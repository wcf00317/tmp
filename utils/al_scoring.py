# æ–‡ä»¶å: wcf00317/alrl/alrl-reward_model/utils/al_scoring.py


import torch
import torch.nn.functional as F
from tqdm import tqdm
import numpy as np

# ğŸ‘‡ å¼•å…¥ feature_extractor
from utils.feature_extractor import UnifiedFeatureExtractor


def precompute_data_for_scoring(args, model, video_indices, train_set, batch_size=32, device="cuda"):
    """
    ä½¿ç”¨ UnifiedFeatureExtractor ç»Ÿä¸€æå– embeddingï¼Œé¿å…å’Œæ¨¡å‹æ¥å£è€¦åˆã€‚
    """
    model.eval()
    print("æ­£åœ¨ä¸ºæ‰€æœ‰è¯„åˆ†å‡½æ•°é¢„è®¡ç®—æ‰€éœ€æ•°æ®...")

    # âœ… å®ä¾‹åŒ–ç‰¹å¾æå–å™¨
    ufe = UnifiedFeatureExtractor(args)

    all_embeddings, all_probs = [], []
    all_fast_embeds, all_slow_embeds = [], []
    all_view2_embeds = []

    # 1. æ‰¹é‡å¤„ç†æœªæ ‡æ³¨æ•°æ®
    with torch.no_grad():
        for i in tqdm(range(0, len(video_indices), batch_size), desc="é¢„è®¡ç®—æœªæ ‡æ³¨æ•°æ®"):
            batch_indices = video_indices[i:i + batch_size]

            # âœ… ç»Ÿä¸€ç”¨ feature_extractor æå–
            features, probs = ufe.get_embeddings_and_probs(batch_indices, model, train_set)

            all_embeddings.append(features.cpu())
            all_probs.append(probs.cpu())

            # âš¡ å¿«/æ…¢é€Ÿç‰ˆæœ¬ä»éœ€é€ä¸ª clip æ„é€ 
            if getattr(args, 'use_temporal_consistency_feature', False):
                fast_clips, slow_clips = [], []
                for vid_idx in batch_indices:
                    fast, slow = train_set.get_video_multi_speed(vid_idx, fast_frames=16, slow_frames=8)
                    fast_clips.append(fast)
                    #slow_clips.append(slow.repeat_interleave(2, dim=2))
                    slow_upsampled = F.interpolate(slow, size=(16, slow.shape[3], slow.shape[4]), mode='trilinear',
                                                   align_corners=False)
                    slow_clips.append(slow_upsampled)

                fast_features, _ = ufe.get_embeddings_and_probs(batch_indices, model, train_set, video_tensors=fast_clips)
                slow_features, _ = ufe.get_embeddings_and_probs(batch_indices, model, train_set, video_tensors=slow_clips)

                all_fast_embeds.append(fast_features.cpu())
                all_slow_embeds.append(slow_features.cpu())

            if getattr(args, 'use_cross_view_consistency_feature', False):
                view2_clips = [train_set.get_video_augmented_views(vid_idx)[1] for vid_idx in batch_indices]
                view2_features, _ = ufe.get_embeddings_and_probs(batch_indices, model, train_set,
                                                                 video_tensors=view2_clips)
                all_view2_embeds.append(view2_features.cpu())

    # 2. å·²æ ‡æ³¨æ•°æ®
    labeled_indices = list(train_set.labeled_video_ids)
    labeled_embeddings = []
    if labeled_indices:
        with torch.no_grad():
            for i in tqdm(range(0, len(labeled_indices), batch_size), desc="é¢„è®¡ç®—å·²æ ‡æ³¨åµŒå…¥"):
                batch_indices = labeled_indices[i:i + batch_size]

                # âœ… ç»Ÿä¸€ç”¨ feature_extractor
                features, _ = ufe.get_embeddings_and_probs(batch_indices, model, train_set)
                labeled_embeddings.append(features.cpu())

    return {
        'embeddings': torch.cat(all_embeddings, dim=0) if all_embeddings else torch.empty(0),
        'probs': torch.cat(all_probs, dim=0) if all_probs else torch.empty(0),
        'labeled_embeddings': torch.cat(labeled_embeddings, dim=0) if labeled_embeddings else None,
        'fast_embeds': torch.cat(all_fast_embeds, dim=0) if all_fast_embeds else torch.empty(0),
        'slow_embeds': torch.cat(all_slow_embeds, dim=0) if all_slow_embeds else torch.empty(0),
        'view2_embeds': torch.cat(all_view2_embeds, dim=0) if all_view2_embeds else torch.empty(0)
    }


# --- ä»¥ä¸‹æ˜¯åŸºäºé¢„è®¡ç®—æ•°æ®çš„è¯„åˆ†å‡½æ•° ---

def compute_entropy_score(precomputed_data):
    """(1) åŸºäºé¢„è®¡ç®—çš„æ¦‚ç‡è®¡ç®—ç†µåˆ†æ•°ï¼ˆå¯¹åº” use_statistical_featuresï¼‰ã€‚"""
    probs = precomputed_data['probs']
    if probs.shape[0] == 0: return torch.empty(0)
    return -torch.sum(probs * torch.log(probs + 1e-8), dim=1)

def compute_diversity_score(precomputed_data):
    """åŸºäº batch å†… embeddings çš„å¹³å‡ç›¸ä¼¼åº¦è®¡ç®—å¤šæ ·æ€§ï¼ˆç›¸ä¼¼åº¦è¶Šé«˜ï¼Œå¤šæ ·æ€§è¶Šä½ï¼‰ã€‚"""
    embeddings = precomputed_data['embeddings']
    if embeddings.shape[0] < 2:
        return torch.zeros(embeddings.shape[0])

    normed_embeds = F.normalize(embeddings, dim=1)
    sim_matrix = torch.matmul(normed_embeds, normed_embeds.T)  # [N, N] cosine similarity
    sim_matrix.fill_diagonal_(0.0)  # å»æ‰è‡ªç›¸ä¼¼
    mean_sim = sim_matrix.mean(dim=1)
    diversity_scores = 1.0 - mean_sim   # è¶Šä½è¡¨ç¤ºè¶Šç›¸ä¼¼ï¼Œè¶Šé«˜è¡¨ç¤ºæ›´ diverse
    return diversity_scores


def compute_representativeness_score(precomputed_data):
    """(3) åŸºäºé¢„è®¡ç®—çš„åµŒå…¥è®¡ç®—ä»£è¡¨æ€§åˆ†æ•°ï¼ˆåˆ°è´¨å¿ƒçš„ä½™å¼¦ç›¸ä¼¼åº¦ï¼‰ã€‚"""
    embeddings = precomputed_data['embeddings']
    if embeddings.shape[0] == 0: return torch.empty(0)
    centroid = torch.mean(embeddings, dim=0, keepdim=True)
    return F.cosine_similarity(embeddings, centroid.squeeze(0), dim=1)

def compute_prediction_margin_score(precomputed_data):
    """(4) åŸºäºé¢„è®¡ç®—çš„æ¦‚ç‡è®¡ç®—é¢„æµ‹è¾¹é™…åˆ†æ•°ã€‚"""
    probs = precomputed_data['probs']
    if probs.shape[0] == 0: return torch.empty(0)
    sorted_probs, _ = torch.sort(probs, dim=1, descending=True)
    return 1.0 - (sorted_probs[:, 0] - sorted_probs[:, 1])

def compute_labeled_distance_score(precomputed_data):
    """(5) åŸºäºé¢„è®¡ç®—çš„åµŒå…¥è®¡ç®—ä¸å·²æ ‡æ³¨é›†çš„æœ€å°è·ç¦»åˆ†æ•°ã€‚"""
    embeddings = precomputed_data['embeddings']
    labeled_embeddings = precomputed_data['labeled_embeddings']
    if labeled_embeddings is None or labeled_embeddings.shape[0] == 0:
        return torch.zeros(embeddings.shape[0])
    dist_matrix = torch.cdist(embeddings, labeled_embeddings)
    scores, _ = torch.min(dist_matrix, dim=1)
    return scores

def compute_neighborhood_density_score(precomputed_data, k=10):
    """åŸºäºé¢„è®¡ç®—çš„ embeddings è®¡ç®—é‚»åŸŸå¯†åº¦åˆ†æ•°ã€‚
    åˆ†æ•°å®šä¹‰ä¸ºæ ·æœ¬ä¸å…¶ k è¿‘é‚»çš„å¹³å‡è·ç¦»çš„å€’æ•°ã€‚
    """
    embeddings = precomputed_data['embeddings']
    n = embeddings.shape[0]
    if n <= k:
        return torch.zeros(n)

    dist_matrix = torch.cdist(embeddings, embeddings, p=2)
    dist_matrix.fill_diagonal_(float('inf'))  # å»æ‰è‡ªè·ç¦» 0

    knn_dists = torch.topk(dist_matrix, k, largest=False, dim=1).values  # [N, k]
    mean_knn_dist = knn_dists.mean(dim=1)  # æ¯ä¸ªæ ·æœ¬çš„å¹³å‡kNNè·ç¦»
    return 1.0 / (1.0 + mean_knn_dist)


def compute_temporal_consistency_score(precomputed_data):
    """(7) åŸºäºé¢„è®¡ç®—çš„å¿«æ…¢é€ŸåµŒå…¥è®¡ç®—æ—¶é—´ä¸€è‡´æ€§åˆ†æ•°ã€‚"""
    fast_embeds = precomputed_data['fast_embeds']
    slow_embeds = precomputed_data['slow_embeds']
    if fast_embeds.shape[0] == 0: return torch.empty(0)
    return 1.0 - F.cosine_similarity(fast_embeds, slow_embeds, dim=1)


# --- ä»¥ä¸‹æ˜¯éœ€è¦ç‹¬ç«‹è®¡ç®—çš„è¯„åˆ†å‡½æ•° ---

def compute_bald_score(model, video_indices, train_set, mc_dropout_iterations=10, batch_size=16):
    """ä½¿ç”¨ MC-Dropout è¿‘ä¼¼è®¡ç®— BALD åˆ†æ•°ã€‚"""
    model.eval()
    for m in model.modules():
        if m.__class__.__name__.startswith('Dropout'):
            m.train()

    print(f"æ­£åœ¨ä½¿ç”¨ MC-Dropout (T={mc_dropout_iterations}) è®¡ç®— BALD åˆ†æ•°...")
    all_bald_scores = []
    with torch.no_grad():
        for i in tqdm(range(0, len(video_indices), batch_size), desc="è®¡ç®— BALD åˆ†æ•°"):
            batch_indices = video_indices[i:i + batch_size]
            clips = torch.stack([train_set.get_video(idx) for idx in batch_indices], dim=0).cuda()

            batch_probs_mc = [F.softmax(model.cls_head(model.extract_feat(clips)[0]), dim=1).unsqueeze(0) for _ in
                              range(mc_dropout_iterations)]
            batch_probs_mc = torch.cat(batch_probs_mc, dim=0)

            mean_probs = torch.mean(batch_probs_mc, dim=0)
            entropy_of_mean = -torch.sum(mean_probs * torch.log(mean_probs + 1e-8), dim=1)
            mean_of_entropies = torch.mean(-torch.sum(batch_probs_mc * torch.log(batch_probs_mc + 1e-8), dim=2), dim=0)

            bald_scores = entropy_of_mean - mean_of_entropies
            all_bald_scores.append(bald_scores.cpu())

    return torch.cat(all_bald_scores, dim=0) if all_bald_scores else torch.empty(0)

def compute_egl_score(model, video_indices, train_set, device="cuda"):
    """æ ‡å‡† EGLï¼ˆå®Œå…¨å®šä¹‰ç‰ˆï¼‰ï¼šåœ¨ GPU ä¸Šè¿è¡Œ"""
    model.eval()
    print("æ­£åœ¨è®¡ç®—æ ‡å‡† EGL åˆ†æ•°ï¼ˆé€æ ·æœ¬é€ç±»åˆ«æ¢¯åº¦ï¼‰...")

    egl_scores = []
    last_layer = model.cls_head
    num_classes = last_layer.out_features

    for idx in tqdm(video_indices, desc="EGL per-sample"):
        clip = train_set.get_video(idx).unsqueeze(0).to(device)
        features = model.extract_feat(clip)[0]
        logits = last_layer(features)
        probs = F.softmax(logits, dim=1).detach().squeeze(0)  # [C]

        sample_score = torch.tensor(0.0, device=device)  # ä¿æŒåœ¨ GPU
        for y in range(num_classes):
            model.zero_grad()
            loss = F.cross_entropy(logits, torch.tensor([y], device=device), reduction='sum')
            loss.backward(retain_graph=True)

            params = [p for p in last_layer.parameters() if p.grad is not None]
            if params:
                all_grads = torch.cat([p.grad.flatten() for p in params])
                grad_norm = all_grads.norm(p=2)
                sample_score += probs[y] * grad_norm

        egl_scores.append(sample_score)

    # ä¸€æ¬¡æ€§è½¬å› CPUï¼Œé¿å…é¢‘ç¹åŒæ­¥
    return torch.stack(egl_scores).detach().cpu()

def compute_egl_dynamic_score_optimized(model, video_indices, train_set, prob_threshold=0.95, batch_size=16):
    """
    åŠ¨æ€ Top-K EGL (ä¼˜åŒ–ç‰ˆ): æ¯ä¸ªæ ·æœ¬åªåšä¸€æ¬¡ forward+backward
    """
    model.eval()
    all_scores = []
    last_layer = model.cls_head

    for i in tqdm(range(0, len(video_indices), batch_size), desc=f"è®¡ç®— åŠ¨æ€-EGL (ä¼˜åŒ–) åˆ†æ•°"):
        batch_indices = video_indices[i:i + batch_size]
        clips = torch.stack([train_set.get_video(idx) for idx in batch_indices], dim=0).cuda()

        # å‰å‘ä¼ æ’­ï¼ˆä¸€æ¬¡ï¼‰
        features = model.extract_feat(clips)[0]
        logits = last_layer(features)
        probs = F.softmax(logits, dim=1)

        batch_scores = []
        for j in range(len(batch_indices)):
            sample_probs = probs[j]
            sorted_probs, sorted_labels = torch.sort(sample_probs, descending=True)

            # åŠ¨æ€ç¡®å®š K
            cumulative = torch.cumsum(sorted_probs, dim=0)
            k = int((cumulative < prob_threshold).sum().item() + 1)

            # æ„é€ åŠ æƒæŸå¤±
            weighted_loss = 0.0
            for p_y, y in zip(sorted_probs[:k], sorted_labels[:k]):
                logit_j = logits[j:j+1]  # å•æ ·æœ¬ logits
                loss_c = F.cross_entropy(logit_j, y.unsqueeze(0), reduction="sum")
                weighted_loss = weighted_loss + p_y * loss_c

            # åå‘ä¼ æ’­ä¸€æ¬¡
            model.zero_grad()
            weighted_loss.backward(retain_graph=True)

            # æå–æ¢¯åº¦èŒƒæ•°
            params = [p for p in last_layer.parameters() if p.grad is not None]
            grads = torch.cat([p.grad.detach().flatten() for p in params])
            grad_norm = grads.norm(p=2).item()
            batch_scores.append(grad_norm)

        all_scores.extend(batch_scores)

    model.zero_grad()
    return torch.tensor(all_scores, dtype=torch.float32).cuda()


def compute_egl_adaptive_topk(model, video_indices, train_set, prob_threshold=0.95, batch_size=16):
    """
    è‡ªé€‚åº” Top-K EGL (ä¼˜åŒ–ç‰ˆ): æ¯ä¸ªæ ·æœ¬åªåšä¸€æ¬¡ forward+backward
    """
    model.eval()
    all_scores = []
    last_layer = model.cls_head

    all_k_values = []
    for i in tqdm(range(0, len(video_indices), batch_size), desc=f"è®¡ç®—è‡ªé€‚åº”K-EGLåˆ†æ•°"):
        batch_indices = video_indices[i:i + batch_size]
        # å‡è®¾ get_video è¿”å›çš„æ˜¯ [1, C, T, H, W]
        clips = torch.stack([train_set[idx][0] for idx in batch_indices], dim=0).cuda()

        # å‰å‘ä¼ æ’­ï¼ˆä¸€æ¬¡ï¼‰
        features = model.extract_feat(clips)[0]
        logits = last_layer(features)
        probs = F.softmax(logits, dim=1)

        for j in range(len(batch_indices)):
            sample_probs = probs[j]
            sorted_probs, sorted_labels = torch.sort(sample_probs, descending=True)

            # åŠ¨æ€ç¡®å®š K
            cumulative = torch.cumsum(sorted_probs, dim=0)
            k = int((cumulative < prob_threshold).sum().item() + 1)
            all_k_values.append(k)

            # æ„é€ åŠ æƒæŸå¤±
            weighted_loss = 0.0
            for p_y, y in zip(sorted_probs[:k], sorted_labels[:k]):
                logit_j = logits[j:j + 1]  # å•æ ·æœ¬ logits
                loss_c = F.cross_entropy(logit_j, y.unsqueeze(0), reduction="sum")
                weighted_loss = weighted_loss + p_y * loss_c

            # åå‘ä¼ æ’­ä¸€æ¬¡
            model.zero_grad()
            # æœ€åä¸€ä¸ªæ ·æœ¬æ— éœ€ä¿ç•™å›¾
            retain_graph = j < len(batch_indices) - 1
            weighted_loss.backward(retain_graph=retain_graph)

            # æå–æ¢¯åº¦èŒƒæ•°
            params = [p for p in last_layer.parameters() if p.grad is not None]
            grad_norm = 0.0
            if params:
                grads = torch.cat([p.grad.detach().flatten() for p in params])
                grad_norm = grads.norm(p=2).item()

            all_scores.append(grad_norm)
    if all_k_values:
        print(f"  - åŠ¨æ€ k å€¼ç»Ÿè®¡: å¹³å‡å€¼={np.mean(all_k_values):.2f}, æœ€å°å€¼={min(all_k_values)}, æœ€å¤§å€¼={max(all_k_values)}")
    model.zero_grad()
    return torch.tensor(all_scores, dtype=torch.float32)

def compute_egl_score_approx(model, video_indices, train_set, batch_size=16):
    """GEMINIç‰ˆæœ¬çš„å·¥ç¨‹è¿‘ä¼¼ï¼Œä¼šå¿«å¾ˆå¤šï¼è®¡ç®—æœ€åä¸€å±‚åˆ†ç±»å¤´çš„æœŸæœ›æ¢¯åº¦é•¿åº¦ (EGL) åˆ†æ•°ã€‚"""
    model.eval()
    print("æ­£åœ¨è®¡ç®—æœŸæœ›æ¢¯åº¦é•¿åº¦ (EGL) åˆ†æ•°...")
    all_egl_scores = []
    last_layer = model.cls_head

    for i in tqdm(range(0, len(video_indices), batch_size), desc="è®¡ç®— EGL åˆ†æ•°"):
        batch_indices = video_indices[i:i + batch_size]
        clips = torch.stack([train_set.get_video(idx) for idx in batch_indices], dim=0).cuda()

        features = model.extract_feat(clips)[0]
        # if features.dim() > 2: features = features.mean(dim=[2, 3, 4])
        logits = last_layer(features)
        pseudo_labels = torch.argmax(logits, dim=1).detach()
        loss = F.cross_entropy(logits, pseudo_labels, reduction='sum')

        model.zero_grad()
        loss.backward()

        params = [p for p in last_layer.parameters() if p.grad is not None]
        if not params:
            grad_norm = 0
        else:
            # 2. å°†æ‰€æœ‰å‚æ•°çš„æ¢¯åº¦å±•å¹³å¹¶æ‹¼æ¥æˆä¸€ä¸ªé•¿å‘é‡
            all_grads = torch.cat([p.grad.detach().flatten() for p in params])
            # 3. è®¡ç®—è¿™ä¸ªé•¿å‘é‡çš„L2èŒƒæ•°
            grad_norm = all_grads.norm(p=2).item()

        avg_grad_norm = grad_norm / len(batch_indices)
        all_egl_scores.extend([avg_grad_norm] * len(batch_indices))

    model.zero_grad()
    return torch.tensor(all_egl_scores, dtype=torch.float32)
def compute_cross_view_consistency_score(precomputed_data):
    view1_embeds = precomputed_data['embeddings']
    view2_embeds = precomputed_data['view2_embeds']
    if view1_embeds.shape[0] == 0 or view2_embeds.shape[0] == 0:
        return torch.empty(0)
    return 1.0 - F.cosine_similarity(view1_embeds, view2_embeds, dim=1)