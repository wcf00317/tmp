# 文件名: c3d_configs/unified_workflow_config.yaml

# ===================================================================
#                       通用配置 (对所有阶段生效)
# ===================================================================
# --- 基本信息 ---
dataset: hmdb51
data_path: ../hmdb51
ckpt_path: ./checkpoints
# 实验名会自动附加时间戳，以确保唯一性
exp_name: hmdb_unified_tournament_workflow
seed: 42

# --- 数据处理 ---
train_batch_size: 32
val_batch_size: 8
workers: 8
clip_len: 16

# --- 模型配置 (MMACTION2) ---
model_cfg_path: ../mmaction2/configs/recognition/c3d/c3d_sports1m-pretrained_8xb30-16x1x1-45e_ucf101-rgb.py
model_ckpt_path: ../pretrained_checkpoints/c3d_sports1m-pretrained_8xb30-16x1x1-45e_ucf101-rgb_20220811-31723200.pth
num_classes: 51
embed_dim: 4096

# ===================================================================
#             阶段 1: 锦标赛数据收集 (Tournament Data Collection)
# ===================================================================
# --- 主动学习预算 ---
initial_labeled_ratio: 0.05
budget_labels: 578 # 总标注预算
num_each_iter: 10  # 每轮主动学习选择的样本数

# --- 极简锦标赛策略专属参数 ---
nomination_ratio_c: 3.0       # 提名池比例 c, c * B
run_sanity_check: false       # 是否开启可选的“理智检查”对决
distance_threshold_alpha: 0.9 # 去冗余步骤中，对中位数距离的缩放因子
egl_strategy: adaptive_k  # 可选项: 'adaptive_k', 'approx', 'standard'

# --- 用于“并集提名”和“奖励模型特征”的开关 ---
# 这些开关将同时控制第一阶段的提名策略 和 生成偏好对时使用的特征
use_statistical_features: true
use_diversity_feature: true
use_representativeness_feature: true
use_prediction_margin_feature: true
use_labeled_distance_feature: true
use_neighborhood_density_feature: true
use_temporal_consistency_feature: true


# ===================================================================
#                       阶段 2: 奖励模型训练 (ALRM Training)
# ===================================================================
reward_model_type: kan # 'kan' 或 'mlp'
kan_grid_size: 5
kan_spline_order: 3
kan_hidden_layers: [8, 4]


# ===================================================================
#                       阶段 3: RL智能体训练 (RL Agent Training)
# ===================================================================
al_algorithm: dqn # 在第三阶段，我们使用DQN智能体

# --- 主分类模型训练设置 ---
optimizer: SGD
lr: 0.0001
weight_decay: 0.0005
momentum: 0.9
gamma: 0.95
epoch_num: 30 # 最终收敛训练的Epoch数
patience: 10   # 早停耐心
al_train_epochs: 15 # AL循环中，每次微调的Epoch数

# --- DQN 智能体训练参数 ---
lr_dqn: 0.0001
gamma_scheduler_dqn: 0.99
rl_pool: 10
rl_buffer: 100
dqn_bs: 5
dqn_gamma: 0.99